{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0211fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook is 1_Networks_Coding_TA_rev2_model-lg\n",
    "\n",
    "#import os\n",
    "import time\n",
    "\n",
    "import pdfplumber\n",
    "#import PyPDF2\n",
    "\n",
    "import import_ipynb\n",
    "from _functions_rev1 import *\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "import spacy_curated_transformers\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ed7c8e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.5270833333333333 minutes ---\n"
     ]
    }
   ],
   "source": [
    "raw_text_cleaned = ''\n",
    "\n",
    "start_time = time.monotonic()\n",
    "\n",
    "with pdfplumber.open(\"S3 Tool Changer.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        # extract text\n",
    "        text = page.extract_text()\n",
    "        # cleaning text\n",
    "        text = clean_text_3(text)\n",
    "        raw_text_cleaned += text\n",
    "\n",
    "print(\"--- %s minutes ---\" % ((time.monotonic() - start_time)/60))\n",
    "#raw_text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cea8d1f3-5206-497d-b162-a1d106fd148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e845fe4-26a5-46ee-907b-d7ec7f1ecdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.1879999998491257 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.monotonic()\n",
    "\n",
    "#doc_ref = nlp(raw_text)\n",
    "doc = nlp(raw_text_cleaned)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.monotonic() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9771290-4c5e-456d-9a3f-67cbfe6747c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence_spans = []\n",
    "for sent in doc.sents:\n",
    "    sentence_spans.append(sent)\n",
    "#sentence_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38218b81-b806-4061-bafb-cbd074d79ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = r'(?=.*\\b(tooling|tool)\\b)(?=.*\\b(adaptor|adaptors)\\b)(?!.*\\b(assembly|assemblies|assemblyÂ¼s)\\b)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93bc9f5f-ea0f-414b-b3c5-d4984f730162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_matched_sentss = []\n",
    "counter=0\n",
    "for item in sentence_spans:\n",
    "    if re.search(phrase, item.text, re.IGNORECASE) or re.search(phrase, item.text, re.IGNORECASE) or re.search(phrase, item.text, re.IGNORECASE):\n",
    "        #print(counter,\"\\n\",item)\n",
    "        counter+=1\n",
    "        list_matched_sentss.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "606e116f-9d0b-4ecd-aa27-122847d5a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_matched_sents = list_matched_sentss\n",
    "#filtration for Tooling Adaptor sents\n",
    "list_matched_sents[2] = list_matched_sents[2][4:]\n",
    "list_matched_sents[7] = list_matched_sents[7][2:]\n",
    "list_matched_sents[8] = list_matched_sents[8][4:]\n",
    "list_matched_sents = list_matched_sents[:-2]\n",
    "list_matched_sents = list_matched_sents[2:]\n",
    "\n",
    "#list_matched_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74938ff7-f543-46a8-bd96-d0c159226dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.render(list_matched_sents[0], style=\"dep\", jupyter=True, options={'distance':80})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8407f978-7d61-4f4b-b094-cc53deceb550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionMaker(sent):\n",
    "    wordsInSents = {}\n",
    "    \n",
    "    for token in sent:\n",
    "        if spacy.explain(token.pos_) in ('noun', 'pronoun', 'proper noun'):\n",
    "            wordsInSents[token.text] = 'object'\n",
    "        if spacy.explain(token.pos_) in ('particle', 'auxiliary', 'verb'):\n",
    "            wordsInSents[token.text] = 'action'\n",
    "        if spacy.explain(token.pos_) in ('adposition', 'subordinating conjunction', 'coordinating conjunction'):\n",
    "            wordsInSents[token.text] = 'positioning'\n",
    "        if spacy.explain(token.pos_) in ('determiner', 'adverb', 'adjective', 'numeral'):\n",
    "            wordsInSents[token.text] = 'determinant'\n",
    "    \n",
    "    dummy = {}\n",
    "    removals = []\n",
    "    for i in range(1,len(wordsInSents)):\n",
    "        if list(wordsInSents.values())[i] != list(wordsInSents.values())[i-1]:\n",
    "            dummy[list(wordsInSents.keys())[i-1]] = list(wordsInSents.values())[i-1]\n",
    "        else:\n",
    "            dummy[list(wordsInSents.keys())[i-1]+' '+list(wordsInSents.keys())[i]] = list(wordsInSents.values())[i]\n",
    "            removals.append(i)\n",
    "    \n",
    "    if list(wordsInSents.values())[-1] == list(wordsInSents.values())[-2]:\n",
    "        dummy[list(wordsInSents.keys())[-2]+' '+list(wordsInSents.keys())[-1]] = list(wordsInSents.values())[-1]\n",
    "        dummy.pop(list(wordsInSents.keys())[-2], None)\n",
    "    else:\n",
    "        dummy[list(wordsInSents.keys())[-1]] = list(wordsInSents.values())[-1]\n",
    "    \n",
    "    for k in removals:\n",
    "        dummy.pop(list(wordsInSents.keys())[k], None)\n",
    "    \n",
    "    wordsInSents = dummy\n",
    "    \n",
    "    #for i in range(1, len(wordsInSents)):\n",
    "    #    if list(wordsInSents.keys())[i] == list(wordsInSents.keys())[i-1][-len(list(wordsInSents.keys())[i]):]:\n",
    "    #        wordsInSents.pop(list(wordsInSents.keys())[i], None)\n",
    "    \n",
    "    return wordsInSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a977de66-f2d8-4e7e-8dba-b7c3c5d13bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_wordsInSent_TA = []\n",
    "for item in list_matched_sents:\n",
    "    list_of_wordsInSent.append(decisionMaker(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c9cf5ba-9717-4af5-9c3b-6b610f6a1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list_of_wordsInSent_TA.pkl', 'wb') as file:\n",
    "     pickle.dump(list_of_wordsInSent_TA,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7604c7e8-1e19-4197-9dae-3f67f6859c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_wordsInSent_TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e8e461-7354-4ce0-9748-408a33b97740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('saved_list.pkl', 'wb') as f:\n",
    "#    pickle.dump(list_of_dicts, f)\n",
    "        \n",
    "#with open('saved_list.pkl', 'rb') as f:\n",
    "#    loaded_list_of_dicts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5482ed28-392c-45d9-8f19-f69bf263876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bboxes(word, table_bbox):\n",
    "    \"\"\"\n",
    "    Check whether word is inside a table bbox.\n",
    "    \"\"\"\n",
    "    l = word['x0'], word['top'], word['x1'], word['bottom']\n",
    "    r = table_bbox\n",
    "    return l[0] > r[0] and l[1] > r[1] and l[2] < r[2] and l[3] < r[3]\n",
    "\n",
    "tables = []\n",
    "counter = 0\n",
    "with pdfplumber.open(\"_inputs/S3 Tool Changer.pdf\") as pdf:\n",
    "    lines = [[] for x in range(len(pdf.pages))]\n",
    "    for page in pdf.pages:        \n",
    "        tables.append(page.find_tables())\n",
    "        table_bboxes = [i.bbox for i in tables[counter]]\n",
    "        tables[counter] = [{'table': i.extract(), 'doctop': i.bbox[1]} for i in tables[counter]]\n",
    "        non_table_words = [word for word in page.extract_words() if not any(\n",
    "            [check_bboxes(word, table_bbox) for table_bbox in table_bboxes])]\n",
    "\n",
    "        for cluster in pdfplumber.utils.cluster_objects(non_table_words+tables[counter], 'doctop', tolerance=5):\n",
    "            if 'text' in cluster[0]:\n",
    "                lines[counter].append(' '.join([i['text'] for i in cluster]))\n",
    "            elif 'table' in cluster[0]:\n",
    "                lines[counter].append(cluster[0]['table'])\n",
    "        counter+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
